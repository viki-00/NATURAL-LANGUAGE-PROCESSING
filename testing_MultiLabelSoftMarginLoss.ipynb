{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"V28","machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"TPU"},"cells":[{"cell_type":"markdown","source":["# Test Phase"],"metadata":{"id":"3mVdzMVNbMDT"}},{"cell_type":"markdown","source":["<h2> Load data"],"metadata":{"id":"o2vObanYcbnW"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"CoLN8WtH9OrA"},"outputs":[],"source":["import pandas as pd\n","from google.colab import drive\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","import logging\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import DataLoader\n","from tqdm import tqdm\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n","import json\n","import torch.nn.functional as F\n","import os\n","from torch.utils.data import Dataset\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n"]},{"cell_type":"code","source":["# load google drive to see the files in google drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8AT2Qi2kbUJ4","executionInfo":{"status":"ok","timestamp":1720600303465,"user_tz":-120,"elapsed":17788,"user":{"displayName":"ProjectToxic","userId":"08112883148140583745"}},"outputId":"d6912b91-3696-4638-9d0a-8278d622a244"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["directory = '2024-07-03_10-03-02' ### ATTENZIONE!!! ALWAYS CHANGE THIS!!!!###\n","\n","path = '/content/drive/MyDrive/Progetto-Vascon/DataLog'\n","save_dir_base = os.path.join(path, directory)\n","save_dir_bert = os.path.join(path, \"bert_sequence_classification_trained\")"],"metadata":{"id":"eNczRu_fbhen"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["<h2> GET the BERT embeddings from drive"],"metadata":{"id":"wPr91j769_V8"}},{"cell_type":"code","source":["import pickle\n","import os\n","import numpy as np\n","\n","# Define filenames for each of the embeddings\n","file_words_test = \"embeddings_words_test.pkl\"\n","file_cls_test = \"embedding_cls_test.pkl\"\n","\n","# Load the embeddings from the specified directory\n","with open(os.path.join(path, 'embeddings', file_words_test), \"rb\") as f:\n","    embeddings_words_test = pickle.load(f)\n","\n","with open(os.path.join(path, 'embeddings', file_cls_test), \"rb\") as f:\n","    embedding_cls_test = pickle.load(f)\n","\n","y_test = np.load(os.path.join(path, 'embeddings', 'y_test.npy'))\n","\n","print(\"Embeddings loaded successfully.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fk5YbuHe9_eP","executionInfo":{"status":"ok","timestamp":1720600542033,"user_tz":-120,"elapsed":234013,"user":{"displayName":"ProjectToxic","userId":"08112883148140583745"}},"outputId":"af05ce2b-1f45-4a43-c9d5-35e359e6c7ff"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Embeddings loaded successfully.\n"]}]},{"cell_type":"markdown","source":["# CLASS Model! ALWAYS CHANGE!\n"," modificare in base al criterion"],"metadata":{"id":"dtin4yRelBqY"}},{"cell_type":"code","source":["class CustomDatasetForCLSToken(Dataset):\n","    def __init__(self, data, targets, cls_tokens):\n","        self.data = data\n","        self.targets = targets\n","        self.cls_tokens = cls_tokens\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def __getitem__(self, idx):\n","        sample = self.data[idx]\n","        target = self.targets[idx]\n","        cls_token = self.cls_tokens[idx]\n","\n","        return sample, target, cls_token\n","\n","\n","custom_test_dataset = CustomDatasetForCLSToken(embeddings_words_test, y_test, embedding_cls_test)"],"metadata":{"id":"K9fEfwok8tuV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","\n","\n","class Attention(nn.Module):\n","    def __init__(self, hidden_dim):\n","        \"\"\"\n","        Initialize the Attention mechanism.\n","        Args:\n","            hidden_dim (int): The number of expected features in the input.\n","        \"\"\"\n","        super(Attention, self).__init__()\n","        self.hidden_dim = hidden_dim\n","        self.attn = nn.Linear(hidden_dim, 1)\n","        self.init_weights()\n","\n","    def init_weights(self):\n","        \"\"\"\n","        Initialize weights for the attention layer.\n","        \"\"\"\n","        nn.init.xavier_uniform_(self.attn.weight)\n","        if self.attn.bias is not None:\n","            nn.init.constant_(self.attn.bias, 0)\n","\n","    def forward(self, rnn_output):\n","        \"\"\"\n","        Forward pass for the attention mechanism.\n","        Args:\n","            rnn_output (torch.Tensor): Output from the RNN layer.\n","        Returns:\n","            torch.Tensor: Attention weights.\n","        \"\"\"\n","        energy = torch.tanh(self.attn(rnn_output))\n","        energy = energy.squeeze(-1)\n","        attention_weights = F.softmax(energy, dim=1)\n","        return attention_weights\n","\n","class SentimentClassifierWithSoftAttention(nn.Module):\n","    def __init__(\n","        self,\n","        embedding_dim=768,\n","        hidden_dim=256,\n","        output_dim=6,\n","        n_layers=1,\n","        bidirectional=True,\n","        dropout=0.0,\n","        rnn_type='LSTM',\n","    ):\n","        \"\"\"\n","        Initialize the SentimentClassifierWithSoftAttention model.\n","        Args:\n","            embedding_dim (int): Dimension of the input embeddings.\n","            hidden_dim (int): Dimension of the hidden layer.\n","            output_dim (int): Dimension of the output layer.\n","            n_layers (int): Number of recurrent layers.\n","            bidirectional (bool): If True, use a bidirectional RNN.\n","            dropout (float): Dropout probability.\n","            rnn_type (str): Type of RNN to use ('LSTM' or 'GRU').\n","        \"\"\"\n","        super().__init__()\n","        self.embedding_dim = embedding_dim\n","        self.hidden_dim = hidden_dim\n","        self.output_dim = output_dim\n","        self.n_layers = n_layers\n","        self.bidirectional = bidirectional\n","        self.dropout = dropout\n","        self.rnn_type = rnn_type\n","\n","        self.attention = Attention(hidden_dim * 2 if bidirectional else hidden_dim)\n","\n","        if rnn_type == 'LSTM':\n","            self.rnn = nn.LSTM(\n","                embedding_dim,\n","                hidden_dim,\n","                num_layers=n_layers,\n","                bidirectional=bidirectional,\n","                dropout=dropout if n_layers > 1 else 0,\n","                batch_first=True,\n","            )\n","        elif rnn_type == 'GRU':\n","            self.rnn = nn.GRU(\n","                embedding_dim,\n","                hidden_dim,\n","                num_layers=n_layers,\n","                bidirectional=bidirectional,\n","                dropout=dropout if n_layers > 1 else 0,\n","                batch_first=True,\n","            )\n","        else:\n","            raise ValueError(\"Choose a valid RNN type: LSTM or GRU\")\n","\n","        self.fc = nn.Sequential(\n","            nn.Linear(hidden_dim * 2 if bidirectional else hidden_dim, hidden_dim),\n","            nn.ReLU(),\n","            nn.Linear(hidden_dim, output_dim)\n","        )\n","\n","        self.fc_cls = nn.Sequential(\n","            nn.Linear(hidden_dim * 2 + embedding_dim if bidirectional else hidden_dim + embedding_dim, hidden_dim),\n","            nn.ReLU(),\n","            nn.Linear(hidden_dim, output_dim)\n","        )\n","\n","        self.init_weights()\n","\n","    def init_weights(self):\n","        \"\"\"\n","        Initialize weights for the RNN and fully connected layers.\n","        \"\"\"\n","        for name, param in self.rnn.named_parameters():\n","            if 'weight' in name:\n","                nn.init.xavier_uniform_(param.data)\n","            elif 'bias' in name:\n","                nn.init.constant_(param.data, 0)\n","\n","        for layer in self.fc_cls:\n","            if isinstance(layer, nn.Linear):\n","                nn.init.xavier_uniform_(layer.weight)\n","                if layer.bias is not None:\n","                    nn.init.constant_(layer.bias, 0)\n","\n","    def forward(self, embedded):\n","        \"\"\"\n","        Forward pass for the model.\n","        Args:\n","            embedded (torch.Tensor): Input embeddings.\n","        Returns:\n","            torch.Tensor: Output logits.\n","            torch.Tensor: Attention weights.\n","        \"\"\"\n","        weighted_sum, attention_weights = self.prepare_data(embedded)\n","        dense_outputs = self.fc(weighted_sum)\n","        outputs = torch.sigmoid(dense_outputs)\n","        return outputs, attention_weights  # Return attention weights\n","\n","    def forward_with_cls(self, embedded, cls_token):\n","        \"\"\"\n","        Forward pass for the model with CLS token.\n","        Args:\n","            embedded (torch.Tensor): Input embeddings.\n","            cls_token (torch.Tensor): CLS token.\n","        Returns:\n","            torch.Tensor: Output logits.\n","            torch.Tensor: Attention weights.\n","        \"\"\"\n","        weighted_sum, attention_weights = self.prepare_data(embedded)\n","        cls_token = cls_token.squeeze(1)\n","        weighted_sum_with_cls = torch.cat((weighted_sum, cls_token), dim=1) # concatenate attention weights + cls_token\n","        dense_outputs = self.fc_cls(weighted_sum_with_cls)\n","        outputs = torch.sigmoid(dense_outputs)\n","        return outputs, attention_weights  # Return attention weights\n","\n","    def prepare_data(self, embedded):\n","        \"\"\"\n","        Prepare data by applying RNN and attention mechanism.\n","        Args:\n","            embedded (torch.Tensor): Input embeddings.\n","        Returns:\n","            torch.Tensor: Weighted sum of RNN outputs.\n","            torch.Tensor: Attention weights.\n","        \"\"\"\n","        if len(embedded.shape) != 3:\n","            raise ValueError(\"Input shape must be 3D: (batch_size, seq_len, embedding_dim)\")\n","        rnn_output, _ = self.rnn(embedded)\n","        attention_weights = self.attention(rnn_output)\n","        attention_weights = attention_weights.unsqueeze(1)\n","        weighted = torch.bmm(attention_weights, rnn_output)\n","        weighted_sum = weighted.squeeze(1)\n","        return weighted_sum, attention_weights.squeeze(1)\n","\n","    def get(self):\n","        \"\"\"\n","        Get model parameters as a JSON string.\n","        Returns:\n","            str: JSON string of model parameters.\n","        \"\"\"\n","        params = {\n","            'embedding_dim': self.embedding_dim,\n","            'hidden_dim': self.hidden_dim,\n","            'output_dim': self.output_dim,\n","            'n_layers': self.n_layers,\n","            'bidirectional': self.bidirectional,\n","            'dropout': self.dropout,\n","            'rnn_type': self.rnn_type\n","        }\n","        return json.dumps(params)\n"],"metadata":{"id":"H4vTE-fslLS1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def calculate_metrics(predictions, targets):\n","    num_classes = predictions.shape[1]\n","    class_metrics = {i: {'true_positives': 0, 'true_negatives': 0, 'false_positives': 0, 'false_negatives': 0} for i in range(num_classes)}\n","\n","    for i in range(num_classes):\n","        for pred, target in zip(predictions[:, i], targets[:, i]):\n","            if pred == 1 and target == 1:\n","                class_metrics[i]['true_positives'] += 1\n","            elif pred == 0 and target == 0:\n","                class_metrics[i]['true_negatives'] += 1\n","            elif pred == 1 and target == 0:\n","                class_metrics[i]['false_positives'] += 1\n","            elif pred == 0 and target == 1:\n","                class_metrics[i]['false_negatives'] += 1\n","\n","\n","\n","    return pd.DataFrame(class_metrics).transpose()\n","\n","def calculate_accuracy(metrics):\n","    return (metrics['true_positives'].sum()+metrics['true_negatives'].sum())/metrics.sum().sum()\n","\n","def calculate_precision(metrics):\n","\n","    if (metrics['true_positives'].sum() + metrics['false_positives'].sum()) >0 :\n","      return (metrics['true_positives'].sum())/(metrics['true_positives'].sum() + metrics['false_positives'].sum())\n","    else: return 0\n","\n","def calculate_recall(metrics):\n","\n","    if (metrics['true_positives'].sum() + metrics['false_negatives'].sum()) >0 :\n","      return (metrics['true_positives'].sum())/(metrics['true_positives'].sum() + metrics['false_negatives'].sum())\n","    else: return 0"],"metadata":{"id":"tbFj4kLXlNsC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def calculate_hamming_distance(predictions, targets):\n","    # Ensure predictions and targets are numpy arrays\n","    import numpy as np\n","    if not isinstance(predictions, np.ndarray):\n","        predictions = np.array(predictions)\n","    if not isinstance(targets, np.ndarray):\n","        targets = np.array(targets)\n","\n","    # Check the shape and size of predictions and targets\n","    assert predictions.shape == targets.shape, \"Predictions and targets must have the same shape.\"\n","\n","    # Calculate the Hamming distance\n","    hamming_distance = np.sum(predictions != targets)\n","\n","    return hamming_distance"],"metadata":{"id":"kJd_STz8iQ68"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def calculate_f1_score(precision, recall):\n","\n","    # Calculate F1 score using harmonic mean of precision and recall\n","    if precision + recall == 0:\n","        return 0  # Avoid division by zero, return zero if both precision and recall are zero\n","\n","    f1_score = 2 * (precision * recall) / (precision + recall)\n","\n","    return f1_score"],"metadata":{"id":"4xXyBOMxiRe4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["<h2> Parameters"],"metadata":{"id":"eG3VKKJvlg5w"}},{"cell_type":"code","source":["batch_size = 264\n","thershold = 0.65\n","\n","test_loader = DataLoader(custom_test_dataset, batch_size=batch_size, shuffle=True)\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"],"metadata":{"id":"r7yig_aqlhAT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["<h2> Testing Phase without CLS"],"metadata":{"id":"hn1AUtkNcr_G"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"OwjPO7saXo3B"},"outputs":[],"source":["import time\n","\n","# After training, load the best model for test prediction\n","#best_model = SentimentClassifierWithSoftAttentionWithCLS().to(device)\n","#best_model.load_state_dict(torch.load('best_model_path'))\n","\n","def test_without_cls(model_rnn, test_loader, t=0.5):\n","  model_rnn.eval()\n","\n","  # Use the best model for prediction on the test set\n","  test_predictions = []\n","  target_embeddings = []\n","\n","  start_time = time.time()\n","  with torch.no_grad():\n","      for data, targets, _ in test_loader:\n","          data, targets = data.to(device), targets.to(device) #, cls_tokens.to(device)\n","          #cls_tokens = cls_tokens.view(cls_tokens.shape[0], 1, cls_tokens.shape[1])\n","\n","          outputs, _ = model_rnn.forward(data.to(dtype=torch.float32)) #, cls_tokens.to(dtype=torch.float32))\n","\n","          # Round values above threshold to 1 and set others to 0\n","          predictions = torch.where(outputs > t, torch.tensor(1.0), torch.tensor(0.0))\n","\n","          # predictions = torch.round(torch.sigmoid(outputs))\n","          test_predictions.append(predictions.cpu())\n","          target_embeddings.append(targets.cpu())\n","  end_time = time.time()\n","  elapsed_time = end_time - start_time\n","\n","  # Convert test_predictions list to a single numpy array if needed\n","\n","  predicted_embeddings_np = torch.cat(test_predictions, dim=0).cpu().numpy()\n","  target_embeddings_np = torch.cat(target_embeddings, dim=0).cpu().numpy()\n","\n","  metrics = calculate_metrics(predicted_embeddings_np,target_embeddings_np)\n","  overall_test_accuracy = calculate_accuracy(metrics)\n","  overall_test_precision = calculate_precision(metrics)\n","  overall_test_recall = calculate_recall(metrics)\n","  overall_test_f1_score = calculate_f1_score(overall_test_precision, overall_test_recall)\n","  overall_test_hamming_distance = calculate_hamming_distance(predicted_embeddings_np,target_embeddings_np)\n","\n","  print(\"----\")\n","  print(f\"Test Accuracy: {overall_test_accuracy:.4f}\")\n","  print(f\"Test Precision: {overall_test_precision:.4f}\")\n","  print(f\"Test Recall: {overall_test_recall:.4f}\")\n","  print(f\"Test F1-score: {overall_test_f1_score:.4f}\")\n","  print(f\"Test Hamming Distance: {overall_test_hamming_distance}\")\n","  print(f\"Elapsed time: {elapsed_time:.3f} seconds\")\n","  print(f\"Elapsed average time: {(elapsed_time*1000)/(len(test_loader)*test_loader.batch_size):.2f} ms\")\n","  print(\"\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":26507,"status":"ok","timestamp":1720600664314,"user":{"displayName":"ProjectToxic","userId":"08112883148140583745"},"user_tz":-120},"id":"yG-yG1oNjBDg","outputId":"26e5f9f1-8bd6-4b97-f7c3-ac0e767c60ef"},"outputs":[{"output_type":"stream","name":"stdout","text":["----\n","Test Accuracy: 0.8849\n","Test Precision: 0.8610\n","Test Recall: 0.7345\n","Test F1-score: 0.7927\n","Test Hamming Distance: 5494\n","Elapsed time: 24.751 seconds\n","Elapsed average time: 3.02 ms\n","\n"]}],"source":["filename = 'LSTM/best_model_LSTM.pth'\n","best_model_path_lstm = os.path.join(save_dir_base, filename)\n","\n","best_model_lstm = SentimentClassifierWithSoftAttention(rnn_type='LSTM').to(device)\n","best_model_lstm.load_state_dict(torch.load(best_model_path_lstm))\n","test_without_cls(best_model_lstm, test_loader, t=thershold)"]},{"cell_type":"code","source":["filename = 'GRU/best_model_GRU.pth'\n","best_model_path_gru = os.path.join(save_dir_base, filename)\n","\n","best_model_gru = SentimentClassifierWithSoftAttention(rnn_type='GRU').to(device)\n","best_model_gru.load_state_dict(torch.load(best_model_path_gru))\n","test_without_cls(best_model_gru, test_loader, t=thershold)"],"metadata":{"id":"J_vyHU_mdgvA","executionInfo":{"status":"ok","timestamp":1720600726993,"user_tz":-120,"elapsed":23229,"user":{"displayName":"ProjectToxic","userId":"08112883148140583745"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"8588db5f-6065-4a87-956f-ce46ae59a595"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["----\n","Test Accuracy: 0.8703\n","Test Precision: 0.9172\n","Test Recall: 0.6236\n","Test F1-score: 0.7424\n","Test Hamming Distance: 6189\n","Elapsed time: 21.618 seconds\n","Elapsed average time: 2.64 ms\n","\n"]}]},{"cell_type":"markdown","source":["<h2> Testing Phase with CLS"],"metadata":{"id":"ABXbpXsgczAu"}},{"cell_type":"code","source":["def test_with_cls(model_rnn, test_loader, t=0.5):\n","  model_rnn.eval()\n","\n","  # Use the best model for prediction on the test set\n","  test_predictions = []\n","  target_embeddings = []\n","\n","  start_time = time.time()\n","  with torch.no_grad():\n","      for data, targets, cls_tokens in test_loader:\n","          data, targets, cls_tokens = data.to(device), targets.to(device), cls_tokens.to(device)\n","          cls_tokens = cls_tokens.view(cls_tokens.shape[0], 1, cls_tokens.shape[1])\n","\n","          outputs, _ = model_rnn.forward_with_cls(data.to(dtype=torch.float32), cls_tokens.to(dtype=torch.float32))\n","          predictions = torch.where(outputs > t, torch.tensor(1.0), torch.tensor(0.0)) #torch.round(torch.sigmoid(outputs))\n","          test_predictions.append(predictions.cpu())\n","          target_embeddings.append(targets.cpu())\n","  end_time = time.time()\n","  elapsed_time = end_time - start_time\n","\n","  # Convert test_predictions list to a single numpy array if needed\n","\n","  predicted_embeddings_np = torch.cat(test_predictions, dim=0).cpu().numpy()\n","  target_embeddings_np = torch.cat(target_embeddings, dim=0).cpu().numpy()\n","\n","  metrics = calculate_metrics(predicted_embeddings_np,target_embeddings_np)\n","  overall_test_accuracy = calculate_accuracy(metrics)\n","  overall_test_precision = calculate_precision(metrics)\n","  overall_test_recall = calculate_recall(metrics)\n","  overall_test_f1_score = calculate_f1_score(overall_test_precision, overall_test_recall)\n","  overall_test_hamming_distance = calculate_hamming_distance(predicted_embeddings_np,target_embeddings_np)\n","\n","  print(\"----\")\n","  print(f\"Test Accuracy: {overall_test_accuracy:.4f}\")\n","  print(f\"Test Precision: {overall_test_precision:.4f}\")\n","  print(f\"Test Recall: {overall_test_recall:.4f}\")\n","  print(f\"Test F1-score: {overall_test_f1_score:.4f}\")\n","  print(f\"Test Hamming Distance: {overall_test_hamming_distance}\")\n","  print(f\"Elapsed time: {elapsed_time:.3f} seconds\")\n","  print(f\"Elapsed average time: {(elapsed_time*1000)/(len(test_loader)*test_loader.batch_size):.2f} ms\")\n","  print(\"\")"],"metadata":{"id":"EXa-5LEvbkqW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["filename = 'LSTM_CLS/best_model_LSTM_CLS.pth'\n","best_model_path_lstm_cls = os.path.join(save_dir_base, filename)\n","\n","\n","best_model_lstm_cls = SentimentClassifierWithSoftAttention(rnn_type='LSTM').to(device)\n","best_model_lstm_cls.load_state_dict(torch.load(best_model_path_lstm_cls))\n","test_with_cls(best_model_lstm_cls, test_loader, t=thershold)"],"metadata":{"id":"xINslUmrdmop","executionInfo":{"status":"ok","timestamp":1720600800311,"user_tz":-120,"elapsed":28054,"user":{"displayName":"ProjectToxic","userId":"08112883148140583745"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"727423c9-c029-4b32-e7cc-e6cae3333a13"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["----\n","Test Accuracy: 0.8855\n","Test Precision: 0.8984\n","Test Recall: 0.6968\n","Test F1-score: 0.7849\n","Test Hamming Distance: 5463\n","Elapsed time: 25.370 seconds\n","Elapsed average time: 3.10 ms\n","\n"]}]},{"cell_type":"code","source":["filename = 'GRU_CLS/best_model_GRU_CLS.pth'\n","best_model_path_gru_cls = os.path.join(save_dir_base, filename)\n","\n","best_model_gru_cls = SentimentClassifierWithSoftAttention(rnn_type='GRU').to(device)\n","best_model_gru_cls.load_state_dict(torch.load(best_model_path_gru_cls))\n","test_with_cls(best_model_gru_cls, test_loader, t=thershold)"],"metadata":{"id":"Rt90WNWtdmvz","executionInfo":{"status":"ok","timestamp":1720600859843,"user_tz":-120,"elapsed":22693,"user":{"displayName":"ProjectToxic","userId":"08112883148140583745"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"546042f6-df1b-4bf0-dd55-1454bb54a3b0"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["----\n","Test Accuracy: 0.8876\n","Test Precision: 0.8866\n","Test Recall: 0.7168\n","Test F1-score: 0.7927\n","Test Hamming Distance: 5362\n","Elapsed time: 21.241 seconds\n","Elapsed average time: 2.60 ms\n","\n"]}]}]}